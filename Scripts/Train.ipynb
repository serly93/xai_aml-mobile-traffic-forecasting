{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ac324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 5x5 to predict center cell\n",
    "from utils import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" # Set the gpu card number to use. Use this line if your machine has multiple gpu cards.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or '3' for FATAL logs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b34333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nr = 5 # grid size of nr*nr\n",
    "Capacity_forecasting = 1 # 1: Capacity forecasting, 0: traffic forecasting\n",
    "lookback = 3 # history length\n",
    "alpha = 0.1 # calibration parameter for capacity forecasting\n",
    "\n",
    "if Capacity_forecasting == 1:\n",
    "    damage = 'capacity_forecasting'\n",
    "else:\n",
    "    damage = 'mae'\n",
    "\n",
    "cell = 5060 # center cell\n",
    "cells = get_rows(cell, 21) # returns cell ids in a 21*21 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepCog architecture from: https://github.com/wnlUc3m/deepcog\n",
    "sample_shape = (nr, nr)\n",
    "batch_size = 128\n",
    "num_cluster = 1\n",
    "no_epochs = 150\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "#steps_per_epoch = 40\n",
    "neurons= 32\n",
    "ker_sz= 3\n",
    "\n",
    "def make_nn_model(load_input_shape, lookback, num_cluster):\n",
    "    '''Build DeepCog architecture'''\n",
    "    inputs = tf.keras.layers.Input(shape=(\n",
    "        lookback,  load_input_shape[0],load_input_shape[1],  1))\n",
    "    x = tf.keras.layers.Conv3D(neurons, kernel_size=(ker_sz, ker_sz, ker_sz), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    x = tf.keras.layers.Conv3D(neurons, kernel_size=(ker_sz * 2, ker_sz * 2, ker_sz * 2), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Conv3D(neurons / 2, kernel_size=(ker_sz * 2,ker_sz * 2, ker_sz * 2), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(neurons * 2, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(neurons, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(num_cluster)(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    if Capacity_forecasting == False:\n",
    "        model.compile(optimizer=Adam(0.0005), loss = mae)\n",
    "    else:\n",
    "        model.compile(optimizer=Adam(0.0005), loss = cost_func_more_args(alpha))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c659f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "model = make_nn_model(sample_shape, lookback, num_cluster)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in cells:\n",
    "    print('CELL',cell)\n",
    "    roww = []\n",
    "    dtt=[]\n",
    "    dt = []\n",
    "    t = []\n",
    "    dt_scaled_old=[]\n",
    "    train_dt = []\n",
    "    test_dt = []\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    dt_reshaped = []\n",
    "    roww = get_rows(cell, nr)       \n",
    "    for i in range(0,len(roww)): \n",
    "        rows=roww[i]\n",
    "        dt.append(pd.read_csv('../Datasets/Milan/PerBS/%s.txt.csv' % rows, header=0, sep=',')) \n",
    "        dt[i].columns = ['ID','time','CC','smsin','smsout','callin','callout','internet']\n",
    "        dt[i] = dt[i].replace(r'^\\s*$', np.nan, regex=True)\n",
    "        val=dt[i]['ID'].iloc[0]\n",
    "        dt[i].drop('smsin', inplace=True, axis=1)\n",
    "        dt[i].drop('smsout', inplace=True, axis=1)\n",
    "        dt[i].drop('callin', inplace=True, axis=1)\n",
    "        dt[i].drop('callout', inplace=True, axis=1)\n",
    "        dt[i].drop('ID', inplace=True, axis=1)\n",
    "        dt[i].drop('CC', inplace=True, axis=1)\n",
    "        dt[i]['time'] = pd.to_datetime(dt[i]['time'], unit = 'ms')\n",
    "        dt[i]['internet'] = dt[i]['internet'].astype(float)\n",
    "        dt[i]['internet']= dt[i]['internet'].fillna(0)   \n",
    "        dt[i]=dt[i].resample('10T', on='time')['internet'].sum()\n",
    "        dt[i]=dt[i].reset_index(level=0)\n",
    "        dt[i].rename(columns={\"internet\": \"%s\" %val}, inplace = True)\n",
    "        dt[i].set_index('time', inplace=True)\n",
    "        dtt.append(dt[i])\n",
    "\n",
    "    for i in range(0,len(dtt)):\n",
    "        t.append(dtt[i])\n",
    "\n",
    "    dt=pd.concat(t,axis=1)\n",
    "    dt=dt.fillna(method=\"bfill\")\n",
    "    dt_reshaped=dt.values.reshape([-1,1])\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    dt_scaled_old=scaler.fit_transform(dt_reshaped)\n",
    "    dt_scaled_old=dt_scaled_old.reshape(dt.shape[0],dt.shape[1])\n",
    "    dt_scaled_old=pd.DataFrame(dt_scaled_old)\n",
    "    nr=int(math.sqrt(dt_scaled_old.shape[1]))\n",
    "    nc=int(math.sqrt(dt_scaled_old.shape[1]))\n",
    "    dtt=dt_scaled_old.values.reshape((dt_scaled_old.shape[0], nr, nc))\n",
    "    \n",
    "    # Split train data and test data\n",
    "    train_size = int(len(dtt)*0.8) \n",
    "    train_dt, test_dt = dtt[:train_size],dtt[train_size:]\n",
    "    np.save('../Trained_models/'+'/'+str(damage)+'/Data/test_'+ str(cell)+'.npy', test_dt)\n",
    "\n",
    "    ds1 = []\n",
    "    ds2 = []\n",
    "    ds3 = []\n",
    "    ds4 = []\n",
    "    ds = []\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    tmp3 = []\n",
    "    tmp4 = []\n",
    "    \n",
    "    ds1= tf.keras.preprocessing.timeseries_dataset_from_array(train_dt,targets=None, sequence_length = lookback,\n",
    "                                                              sequence_stride = 1,shuffle = False)\n",
    "    for batch1 in ds1:\n",
    "        tmp1.append(batch1.numpy())\n",
    "\n",
    "    train_X=np.vstack(tmp1)\n",
    "\n",
    "    ds2= tf.keras.preprocessing.timeseries_dataset_from_array(train_dt[:,2,2],targets=None, sequence_length = 1,\n",
    "                                                            sequence_stride=1,shuffle = False,start_index = lookback)\n",
    "\n",
    "    for batch2 in ds2:\n",
    "        tmp2.append(batch2.numpy())\n",
    "\n",
    "    train_Y=np.vstack(tmp2)\n",
    "    train_X=train_X[0:train_Y.shape[0]]\n",
    "\n",
    "    ds3 = tf.keras.preprocessing.timeseries_dataset_from_array(test_dt,targets = None, sequence_length = lookback,\n",
    "                                                             sequence_stride = 1,shuffle = False)\n",
    "    for batch3 in ds3:\n",
    "        tmp3.append(batch3.numpy())\n",
    "    test_X = np.vstack(tmp3)\n",
    "\n",
    "    ds4= tf.keras.preprocessing.timeseries_dataset_from_array(test_dt[:,2,2],targets = None, sequence_length = 1,\n",
    "                                                                 sequence_stride = 1, shuffle = False, start_index = lookback)\n",
    "    for batch4 in ds4:\n",
    "        tmp4.append(batch4.numpy())\n",
    "    test_Y = np.vstack(tmp4)\n",
    "    test_X = test_X[0:test_Y.shape[0]]\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0],train_X.shape[1],train_X.shape[2],train_X.shape[3],1))  \n",
    "    test_X = np.reshape(test_X, (test_X.shape[0],test_X.shape[1],test_X.shape[2],test_X.shape[3],1))\n",
    "\n",
    "    nn = make_nn_model(sample_shape, lookback, num_cluster)\n",
    "    history = nn.fit(train_X, train_Y,batch_size = batch_size, epochs = no_epochs,\n",
    "                verbose = 0, validation_split = validation_split, shuffle = True)\n",
    "    \n",
    "    nn.save('../Trained_models/'+str(damage)+'/Models/mymodel_%d.h5' %cell, save_format='tf')\n",
    "    dt=[]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e0c58788f346b266fbc3647b102f93196dbe6ff6400d04c9c4d9ad5d261b93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
